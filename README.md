# [ICCV2025 Highlight] Where, What, Why: Towards Explainable Driver Attention Prediction

[![Paper](https://img.shields.io/badge/Paper-ArXiv-red)](https://arxiv.org/abs/2506.23088)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

This repository contain the official code and dataset for the paper:

**"Where, What, Why: Towards Explainable Driver Attention Prediction"**  
[Yuchen Zhou\*](https://yuchen2199.github.io/), Jiayu Tang\*, Xiaoyan Xiao, Yueyao Lin, [Linkai Liu](https://liulinkai.github.io/), Zipeng Guo, [Hao Fei](https://haofei.vip/), [Xiaobo Xia](https://xiaoboxia.github.io/), [Chao Gouâ€ ](https://chaogou.github.io/)  
(*Equal contribution, â€ Corresponding author)  
ğŸ“ Sun Yat-sen University & National University of Singapore  
ğŸ“„ [arXiv 2506.23088](https://arxiv.org/abs/2506.23088)

---

## ğŸ” Overview

This work introduces **Explainable Driver Attention Prediction (EDAP)** â€” a novel task that jointly predicts:

- ğŸ§­ **Where**: pixel-level driver attention maps  
- ğŸ§© **What**: semantic understanding of attended regions  
- ğŸ§  **Why**: cognitive reasoning behind the attention

We present:

- **WÂ³DA**: The first large-scale dataset with spatial, semantic, and causal annotations across normal, critical, and accident driving scenarios.
- **LLada**: A multimodal large language model (MLLM)-driven architecture that unifies pixel prediction, semantic parsing, and causal reasoning in an end-to-end framework.

---

## ğŸ› ï¸ To-Do List

We are actively building this repository. The full code and cleaned dataset will be released progressively. Stay tuned!

- [âœ”] Initial Code Release
- [ ] Refactored & Cleaned Code
- [ ] Cleaned WÂ³DA Dataset Release
---

## Training

### Pre-trained weights

In LLada, we employ the pre-trained LLaVA weights `LLaVA-Lightning-7B-v1-1` , which can be directly downloaded from this [link](https://huggingface.co/mmaaz60/LLaVA-7B-Lightening-v1-1). As to MLLMâ€™s vision backbone, we adopt the `clip-vit-large-patch14`, which can obtained from [link](https://huggingface.co/openai/clip-vit-large-patch14).

### Data Preparation

WÂ³DA Dataset sources from 4 public datasets, [**BDDA**](http://bdd-data.berkeley.edu/download.html), [**DR(eye)VE**](https://aimagelab.ing.unimore.it/imagelab/page.asp?IdPage=8), [**LBW**](https://github.com/Kasai2020/look_both_ways?tab=readme-ov-file), [**DADA-2000**](https://github.com/JWFangit/LOTVS-DADA).

Upon downloading the WÂ³DA dataset, you need to organize it as follows:

```
â”œâ”€â”€ dataset
â”‚   â”œâ”€â”€ BDDA
â”‚   â”‚   â”œâ”€â”€ training
â”‚   â”‚       â”œâ”€â”€ 0001
â”‚   â”‚       	â””â”€â”€ raw_frames
â”‚   â”‚       	â””â”€â”€ gazemap_frames
â”‚   â”‚       	â””â”€â”€ 000001.json
â”‚   â”‚       	â””â”€â”€ 000002.json
â”‚   â”‚       	â””â”€â”€ 000003.json
â”‚   â”‚       	â””â”€â”€ ......
â”‚   â”‚       â””â”€â”€ 0002
â”‚   â”‚       â””â”€â”€ 0003
â”‚   â”‚       â””â”€â”€ ......
â”‚   â”‚   â””â”€â”€ validation
â”‚   â”‚   â””â”€â”€ test
â”‚   â”œâ”€â”€ DReyeVE
â”‚   â”‚   â”œâ”€â”€ training
â”‚   â”‚   â””â”€â”€ validation
â”‚   â”‚   â””â”€â”€ test
â”‚   â”œâ”€â”€ LBW
â”‚   â”‚   â”œâ”€â”€ training
â”‚   â”‚   â””â”€â”€ validation
â”‚   â”‚   â””â”€â”€ test
â”‚   â”œâ”€â”€ DADA
â”‚   â”‚   â”œâ”€â”€ training
â”‚   â”‚   â””â”€â”€ validation
â”‚   â”‚   â””â”€â”€ test
```

To train the model, please use the following instruction, where `dataset` designates the sub-dataset used for training; `train_sample_rates` specifies the sampling rate of training samples for each sub-dataset.

```
deepspeed --master_port=24999 train_ds.py \
  --version="PATH_TO_LLaVA" \
  --vision-tower="PATH_TO_CLIP" \
  --dataset_dir="PATH_TO_W3DA" \
  --log_base_dir="PATH_TO_LOG" \
  --dataset="BDDA||DReyeVE||LBW||DADA" \
  --train_sample_rates="8,5,2,7" \
  --exp_name="Attn-7b"
```

When training is finished, you can obtain the full model using the `zero_to_fp32.py` script which is automatically generated by `deepspeed` engine within the folder `./runs/Attn-7b/ckpt_model`.

```
cd ./runs/Attn-7b/ckpt_model && python zero_to_fp32.py . ../pytorch_model.bin
```

### Merge LoRA weight

Merge the LoRA weights of `pytorch_model.bin`, save the resulting model into your desired path in the Hugging Face format:

```
CUDA_VISIBLE_DEVICES="" python3 merge_lora_weights_and_save_hf_model.py \
  --version="PATH_TO_LLaVA" \
  --vision-tower="PATH_TO_CLIP" \
  --weight="Attn-7b/pytorch_model.bin" \
  --save_path="./ATTN-7B"
```

## Validation

To validate the finetuned model, please use the following instruction, where `val_dataset` designates the sub-datasets to evaluate. `eval_only` is required to enable the evaluation process.

```
deepspeed --master_port=24999 train_ds.py \
  --version="PATH_TO_LLada" \
  --vision-tower="PATH_TO_CLIP" \
  --dataset_dir="PATH_TO_W3DA" \
  --log_base_dir="PATH_TO_LOG" \
  --val_dataset="BDDA||DReyeVE||LBW||DADA" \
  --val_batch_size=1 \
  --eval_only
```


## ğŸ“¦ Citation

If you find this work helpful, please cite:

```bibtex
@inproceedings{zhou2025where,
      title={Where, What, Why: Towards Explainable Driver Attention Prediction}, 
      author={Yuchen Zhou and Jiayu Tang and Xiaoyan Xiao and Yueyao Lin and Linkai Liu and Zipeng Guo and Hao Fei and Xiaobo Xia and Chao Gou},
      year={2025},
      eprint={2506.23088},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2506.23088}, 
}
```

## Acknowledgement

-  This work is built upon the [LLaVA](https://github.com/haotian-liu/LLaVA) and  [LISA](https://github.com/dvlab-research/LISA). 

## ğŸ“¦ Contact
Feel free to reach out for questions or collaborations:

ğŸ“§ Yuchen Zhou: zhouych37@mail2.sysu.edu.cn


